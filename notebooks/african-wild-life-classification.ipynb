{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Arewa Data Science Deep Learning with PyTorch Capstone Project - Group 2\n","\n","## WildLife Classify App\n"]},{"cell_type":"markdown","metadata":{},"source":["This notebook serves as the documentation for the capstone project (Group 2) for the Arewa Data Science Deep Learning with PyTorch Part 1 [course](https://github.com/arewadataScience/ArewaDS-Deep-Learning), instructed by [Mr. Mustapha Abdullahi](https://github.com/mustious). We applied our learning in Computer Vision to develop a model capable of identifying wild animals, utilizing the African Wildlife dataset for this purpose.We also deployed the model on streamlit."]},{"cell_type":"markdown","metadata":{},"source":["We begin by the necessary imports. We use wandb to log all the training metrics."]},{"cell_type":"markdown","metadata":{},"source":["> Densenet was selected due to its smaller size and efficiency compared to Alexnet. We could host the trained model weights on github for deployment on streamlit share. "]},{"cell_type":"markdown","metadata":{},"source":["> Notebook was used in Kaggle on a Kaggle Dataset. All paths need to be properly redefined if one wants to use it in a different enviroment. Same result will be obtained nonetheless. Find it [here](https://www.kaggle.com/code/lukmanaliyuj/african-wild-life-classification) on kaggle"]},{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T12:34:03.211401Z","iopub.status.busy":"2024-02-17T12:34:03.210771Z","iopub.status.idle":"2024-02-17T12:34:03.224684Z","shell.execute_reply":"2024-02-17T12:34:03.223719Z","shell.execute_reply.started":"2024-02-17T12:34:03.211369Z"},"trusted":true},"outputs":[],"source":["# Necessary imports\n","try:\n","  import wandb\n","except:\n","  !pip install -q wandb\n","  import wandb\n","# importing split-folders to split my dataset\n","try:\n","    import splitfolders\n","except:\n","    !pip install split-folders[full]\n","    import splitfolders  \n","# Try to get torchinfo, install it if it doesn't work\n","try:\n","    from torchinfo import summary\n","except:\n","    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n","    !pip install -q torchinfo\n","    from torchinfo import summary\n","import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision\n","from torchvision import transforms,datasets\n","from tqdm.auto import tqdm\n","from typing import Dict, List, Tuple\n","from pathlib import Path\n","import os\n"]},{"cell_type":"markdown","metadata":{},"source":["# Logging into wandb (Weights and Biases)"]},{"cell_type":"markdown","metadata":{},"source":["Next, logging into wandb. This enables us save the model metrics in an easy to retrieve way."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T12:34:06.981186Z","iopub.status.busy":"2024-02-17T12:34:06.980598Z","iopub.status.idle":"2024-02-17T12:34:50.048397Z","shell.execute_reply":"2024-02-17T12:34:50.047451Z","shell.execute_reply.started":"2024-02-17T12:34:06.981157Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["wandb.login()"]},{"cell_type":"markdown","metadata":{},"source":["Setting up device agnostic code. Since we intend to use GPU to train the model."]},{"cell_type":"markdown","metadata":{},"source":["# Setting up device agnostic code"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T12:34:53.574037Z","iopub.status.busy":"2024-02-17T12:34:53.573375Z","iopub.status.idle":"2024-02-17T12:34:53.630667Z","shell.execute_reply":"2024-02-17T12:34:53.629603Z","shell.execute_reply.started":"2024-02-17T12:34:53.574003Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'cuda'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Setup device agnostic code\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"markdown","metadata":{},"source":["We check the type of GPU device available on this kaggle notebook just to be sure."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T12:35:02.450049Z","iopub.status.busy":"2024-02-17T12:35:02.449671Z","iopub.status.idle":"2024-02-17T12:35:03.517909Z","shell.execute_reply":"2024-02-17T12:35:03.516765Z","shell.execute_reply.started":"2024-02-17T12:35:02.450021Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Sat Feb 17 12:35:03 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n","| N/A   43C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T12:35:13.170157Z","iopub.status.busy":"2024-02-17T12:35:13.169768Z","iopub.status.idle":"2024-02-17T12:35:13.175289Z","shell.execute_reply":"2024-02-17T12:35:13.174074Z","shell.execute_reply.started":"2024-02-17T12:35:13.170125Z"},"trusted":true},"outputs":[],"source":["NUM_WORKERS = os.cpu_count()\n"]},{"cell_type":"markdown","metadata":{},"source":["# Creating Utility Functions for Training"]},{"cell_type":"markdown","metadata":{},"source":["Next, is a couple of utility functions to make the training easy. These are functions we were given in the course [material](https://www.learnpytorch.io/). The first is a function for creating dataloaders from image directories (train and test directories)."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T12:35:28.002234Z","iopub.status.busy":"2024-02-17T12:35:28.001600Z","iopub.status.idle":"2024-02-17T12:35:28.010754Z","shell.execute_reply":"2024-02-17T12:35:28.009751Z","shell.execute_reply.started":"2024-02-17T12:35:28.002203Z"},"trusted":true},"outputs":[],"source":["def create_dataloaders(\n","    train_dir: str, \n","    test_dir: str, \n","    transform: transforms.Compose, \n","    batch_size: int, \n","    num_workers: int=NUM_WORKERS\n","):\n","  \"\"\"Creates training and testing DataLoaders.\n","\n","  Takes in a training directory and testing directory path and turns\n","  them into PyTorch Datasets and then into PyTorch DataLoaders.\n","\n","  Args:\n","    train_dir: Path to training directory.\n","    test_dir: Path to testing directory.\n","    transform: torchvision transforms to perform on training and testing data.\n","    batch_size: Number of samples per batch in each of the DataLoaders.\n","    num_workers: An integer for number of workers per DataLoader.\n","\n","  Returns:\n","    A tuple of (train_dataloader, test_dataloader, class_names).\n","    Where class_names is a list of the target classes.\n","    Example usage:\n","      train_dataloader, test_dataloader, class_names = \\\n","        = create_dataloaders(train_dir=path/to/train_dir,\n","                             test_dir=path/to/test_dir,\n","                             transform=some_transform,\n","                             batch_size=32,\n","                             num_workers=4)\n","  \"\"\"\n","  # Use ImageFolder to create dataset(s)\n","  train_data = datasets.ImageFolder(train_dir, transform=transform)\n","  test_data = datasets.ImageFolder(test_dir, transform=transform)\n","\n","  # Get class names\n","  class_names = train_data.classes\n","\n","  # Turn images into data loaders\n","  train_dataloader = DataLoader(\n","      train_data,\n","      batch_size=batch_size,\n","      shuffle=True,\n","      num_workers=num_workers,\n","      pin_memory=True,\n","  )\n","  test_dataloader = DataLoader(\n","      test_data,\n","      batch_size=batch_size,\n","      shuffle=False,\n","      num_workers=num_workers,\n","      pin_memory=True,\n","  )\n","\n","  return train_dataloader, test_dataloader, class_names"]},{"cell_type":"markdown","metadata":{},"source":["The next functions define the train and test steps and a function that combines the entire training in a single function, utilizing all the initial functions. "]},{"cell_type":"markdown","metadata":{},"source":["# Training Function"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T12:35:33.065995Z","iopub.status.busy":"2024-02-17T12:35:33.065644Z","iopub.status.idle":"2024-02-17T12:35:33.091920Z","shell.execute_reply":"2024-02-17T12:35:33.090812Z","shell.execute_reply.started":"2024-02-17T12:35:33.065967Z"},"trusted":true},"outputs":[],"source":["def train_step(model: torch.nn.Module, \n","               dataloader: torch.utils.data.DataLoader, \n","               loss_fn: torch.nn.Module, \n","               optimizer: torch.optim.Optimizer,\n","               device: torch.device) -> Tuple[float, float]:\n","    \"\"\"Trains a PyTorch model for a single epoch.\n","\n","    Turns a target PyTorch model to training mode and then\n","    runs through all of the required training steps (forward\n","    pass, loss calculation, optimizer step).\n","\n","    Args:\n","    model: A PyTorch model to be trained.\n","    dataloader: A DataLoader instance for the model to be trained on.\n","    loss_fn: A PyTorch loss function to minimize.\n","    optimizer: A PyTorch optimizer to help minimize the loss function.\n","    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n","\n","    Returns:\n","    A tuple of training loss and training accuracy metrics.\n","    In the form (train_loss, train_accuracy). For example:\n","\n","    (0.1112, 0.8743)\n","    \"\"\"\n","    # Put model in train mode\n","    model.train()\n","\n","    # Setup train loss and train accuracy values\n","    train_loss, train_acc = 0, 0\n","\n","    # Loop through data loader data batches\n","    for batch, (X, y) in enumerate(dataloader):\n","        # Send data to target device\n","        X, y = X.to(device), y.to(device)\n","\n","        # 1. Forward pass\n","        y_pred = model(X)\n","\n","        # 2. Calculate  and accumulate loss\n","        loss = loss_fn(y_pred, y)\n","        train_loss += loss.item() \n","\n","        # 3. Optimizer zero grad\n","        optimizer.zero_grad()\n","\n","        # 4. Loss backward\n","        loss.backward()\n","\n","        # 5. Optimizer step\n","        optimizer.step()\n","\n","        # Calculate and accumulate accuracy metric across all batches\n","        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n","        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n","\n","    # Adjust metrics to get average loss and accuracy per batch \n","    train_loss = train_loss / len(dataloader)\n","    train_acc = train_acc / len(dataloader)\n","    return train_loss, train_acc\n","\n","def test_step(model: torch.nn.Module, \n","              dataloader: torch.utils.data.DataLoader, \n","              loss_fn: torch.nn.Module,\n","              device: torch.device) -> Tuple[float, float]:\n","    \"\"\"Tests a PyTorch model for a single epoch.\n","\n","    Turns a target PyTorch model to \"eval\" mode and then performs\n","    a forward pass on a testing dataset.\n","\n","    Args:\n","    model: A PyTorch model to be tested.\n","    dataloader: A DataLoader instance for the model to be tested on.\n","    loss_fn: A PyTorch loss function to calculate loss on the test data.\n","    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n","\n","    Returns:\n","    A tuple of testing loss and testing accuracy metrics.\n","    In the form (test_loss, test_accuracy). For example:\n","\n","    (0.0223, 0.8985)\n","    \"\"\"\n","    # Put model in eval mode\n","    model.eval() \n","\n","    # Setup test loss and test accuracy values\n","    test_loss, test_acc = 0, 0\n","\n","    # Turn on inference context manager\n","    with torch.inference_mode():\n","        # Loop through DataLoader batches\n","        for batch, (X, y) in enumerate(dataloader):\n","            # Send data to target device\n","            X, y = X.to(device), y.to(device)\n","\n","            # 1. Forward pass\n","            test_pred_logits = model(X)\n","\n","            # 2. Calculate and accumulate loss\n","            loss = loss_fn(test_pred_logits, y)\n","            test_loss += loss.item()\n","\n","            # Calculate and accumulate accuracy\n","            test_pred_labels = test_pred_logits.argmax(dim=1)\n","            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n","\n","    # Adjust metrics to get average loss and accuracy per batch \n","    test_loss = test_loss / len(dataloader)\n","    test_acc = test_acc / len(dataloader)\n","    return test_loss, test_acc\n","\n","def train(model: torch.nn.Module, \n","          train_dataloader: torch.utils.data.DataLoader, \n","          test_dataloader: torch.utils.data.DataLoader, \n","          optimizer: torch.optim.Optimizer,\n","          loss_fn: torch.nn.Module,\n","          epochs: int,\n","          device: torch.device) -> Dict[str, List[float]]:\n","    \"\"\"Trains and tests a PyTorch model.\n","\n","    Passes a target PyTorch models through train_step() and test_step()\n","    functions for a number of epochs, training and testing the model\n","    in the same epoch loop.\n","\n","    Calculates, prints and stores evaluation metrics throughout.\n","\n","    Args:\n","    model: A PyTorch model to be trained and tested.\n","    train_dataloader: A DataLoader instance for the model to be trained on.\n","    test_dataloader: A DataLoader instance for the model to be tested on.\n","    optimizer: A PyTorch optimizer to help minimize the loss function.\n","    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n","    epochs: An integer indicating how many epochs to train for.\n","    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n","\n","    Returns:\n","    A dictionary of training and testing loss as well as training and\n","    testing accuracy metrics. Each metric has a value in a list for \n","    each epoch.\n","    In the form: {train_loss: [...],\n","              train_acc: [...],\n","              test_loss: [...],\n","              test_acc: [...]} \n","    For example if training for epochs=2: \n","             {train_loss: [2.0616, 1.0537],\n","              train_acc: [0.3945, 0.3945],\n","              test_loss: [1.2641, 1.5706],\n","              test_acc: [0.3400, 0.2973]} \n","    \"\"\"\n","    # Create empty results dictionary\n","    results = {\"train_loss\": [],\n","               \"train_acc\": [],\n","               \"test_loss\": [],\n","               \"test_acc\": []\n","    }\n","\n","    # Loop through training and testing steps for a number of epochs\n","    for epoch in tqdm(range(epochs)):\n","        train_loss, train_acc = train_step(model=model,\n","                                          dataloader=train_dataloader,\n","                                          loss_fn=loss_fn,\n","                                          optimizer=optimizer,\n","                                          device=device)\n","        test_loss, test_acc = test_step(model=model,\n","          dataloader=test_dataloader,\n","          loss_fn=loss_fn,\n","          device=device)\n","\n","        # Print out what's happening\n","        print(\n","          f\"Epoch: {epoch+1} | \"\n","          f\"train_loss: {train_loss:.4f} | \"\n","          f\"train_acc: {train_acc:.4f} | \"\n","          f\"test_loss: {test_loss:.4f} | \"\n","          f\"test_acc: {test_acc:.4f}\"\n","        )\n","        # Log metrics to wandb\n","        wandb.log({\"epoch\": epoch+1,\n","               \"train_loss\": train_loss,\n","               \"train_acc\": train_acc,\n","               \"test_loss\": test_loss,\n","               \"test_acc\": test_acc})\n","\n","        # Update results dictionary\n","        results[\"train_loss\"].append(train_loss)\n","        results[\"train_acc\"].append(train_acc)\n","        results[\"test_loss\"].append(test_loss)\n","        results[\"test_acc\"].append(test_acc)\n","\n","    # Return the filled results at the end of the epochs\n","    return results"]},{"cell_type":"markdown","metadata":{},"source":["We then define the folders for the input and output, using the directory structure in kaggle."]},{"cell_type":"markdown","metadata":{},"source":["# Defining the Directories and Splitting the Images into train-test splits"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T12:36:31.551432Z","iopub.status.busy":"2024-02-17T12:36:31.551058Z","iopub.status.idle":"2024-02-17T12:36:31.556113Z","shell.execute_reply":"2024-02-17T12:36:31.554959Z","shell.execute_reply.started":"2024-02-17T12:36:31.551406Z"},"trusted":true},"outputs":[],"source":["input_folder = \"/kaggle/input/african-wildlife\"\n","output = \"/kaggle/working/\""]},{"cell_type":"markdown","metadata":{},"source":["The splitfolder library is then used to split the images (already in an ImageFolder structure) into train and test splits. "]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T12:36:35.484904Z","iopub.status.busy":"2024-02-17T12:36:35.484567Z","iopub.status.idle":"2024-02-17T12:37:04.905906Z","shell.execute_reply":"2024-02-17T12:37:04.905021Z","shell.execute_reply.started":"2024-02-17T12:36:35.484880Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Copying files: 3008 files [00:29, 102.30 files/s]\n"]}],"source":["# Split with a ratio.\n","# To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.\n","splitfolders.ratio(input_folder, output=output,\n","    seed=1337, ratio=(.8, .2), group_prefix=None, move=False) # default values\n"]},{"cell_type":"markdown","metadata":{},"source":["The train and test directories are then defined from the output from splitfolder. "]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T12:37:42.921577Z","iopub.status.busy":"2024-02-17T12:37:42.921183Z","iopub.status.idle":"2024-02-17T12:37:42.926112Z","shell.execute_reply":"2024-02-17T12:37:42.925125Z","shell.execute_reply.started":"2024-02-17T12:37:42.921546Z"},"trusted":true},"outputs":[],"source":["# Set up train and test dirs\n","train_dir = \"/kaggle/working/train\"\n","test_dir = \"/kaggle/working/val\""]},{"cell_type":"markdown","metadata":{},"source":["# Defining Data Transform"]},{"cell_type":"markdown","metadata":{},"source":["Next thing is to define the data transform we want to do on the images. "]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T12:37:54.963946Z","iopub.status.busy":"2024-02-17T12:37:54.962803Z","iopub.status.idle":"2024-02-17T12:37:54.989258Z","shell.execute_reply":"2024-02-17T12:37:54.988378Z","shell.execute_reply.started":"2024-02-17T12:37:54.963905Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(<torch.utils.data.dataloader.DataLoader at 0x7cf938e51ed0>,\n"," <torch.utils.data.dataloader.DataLoader at 0x7cf938e533d0>,\n"," ['buffalo', 'elephant', 'rhino', 'zebra'])"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# # Create simple transform\n","data_transform = transforms.Compose([ \n","    transforms.Resize((64, 64)),\n","    transforms.ToTensor(),\n","    \n","])\n","# Create training and testing DataLoader's as well as get a list of class names\n","train_dataloader, test_dataloader, class_names = create_dataloaders(train_dir=train_dir,\n","                                                                               test_dir=test_dir,\n","                                                                               transform= data_transform,\n","                                                                               batch_size=32) # set mini-batch size to 32\n","\n","train_dataloader, test_dataloader, class_names"]},{"cell_type":"markdown","metadata":{},"source":["The classes show up nicely. As a reminder, the intention is to build a classifier that classifies any picture into buffalo, elephant, rhino and zebra. "]},{"cell_type":"markdown","metadata":{},"source":["Not knowing how the text files in the image folder will impact training, We remove them in the cell below."]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T12:38:17.727873Z","iopub.status.busy":"2024-02-17T12:38:17.727376Z","iopub.status.idle":"2024-02-17T12:38:17.736885Z","shell.execute_reply":"2024-02-17T12:38:17.735735Z","shell.execute_reply.started":"2024-02-17T12:38:17.727837Z"},"trusted":true},"outputs":[],"source":["# removing all the .txt file\n","datapath = '/kaggle/working/'\n","\n","# Walk through the directory\n","for root, dirs, files in os.walk(datapath):\n","    for file in files:\n","        # Check if the file is a .txt file\n","        if file.endswith('.txt'):\n","            # Construct the full file path\n","            path_to_file = os.path.join(root, file)\n","            # Remove the file\n","            os.remove(path_to_file)\n","            "]},{"cell_type":"markdown","metadata":{},"source":["# Get The Pretrained model\n"]},{"cell_type":"markdown","metadata":{},"source":["Utilizing pretrained models is a popular strategy not only in Computer vision but in all of deep learning. We follow suit by utilizing the densenet201 from torchvision using the code below. "]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T12:38:23.134003Z","iopub.status.busy":"2024-02-17T12:38:23.132995Z","iopub.status.idle":"2024-02-17T12:38:30.770182Z","shell.execute_reply":"2024-02-17T12:38:30.769320Z","shell.execute_reply.started":"2024-02-17T12:38:23.133963Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /root/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n","100%|██████████| 77.4M/77.4M [00:06<00:00, 13.0MB/s]\n"]}],"source":["# Setup the model with pretrained weights and send it to the target device\n","weights = torchvision.models.DenseNet201_Weights.DEFAULT # best available weight\n","densenet = torchvision.models.densenet201(weights=weights).to(device)\n"]},{"cell_type":"markdown","metadata":{},"source":["We then look at the nature of the model using the summary function from the torchinfo library. "]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T12:38:40.334463Z","iopub.status.busy":"2024-02-17T12:38:40.333598Z","iopub.status.idle":"2024-02-17T12:38:41.481790Z","shell.execute_reply":"2024-02-17T12:38:41.480854Z","shell.execute_reply.started":"2024-02-17T12:38:40.334431Z"},"trusted":true},"outputs":[{"data":{"text/plain":["===================================================================================================================\n","Layer (type:depth-idx)                   Input Shape               Output Shape              Trainable\n","===================================================================================================================\n","DenseNet                                 [32, 3, 224, 224]         [32, 1000]                True\n","├─Sequential: 1-1                        [32, 3, 224, 224]         [32, 1920, 7, 7]          True\n","│    └─Conv2d: 2-1                       [32, 3, 224, 224]         [32, 64, 112, 112]        True\n","│    └─BatchNorm2d: 2-2                  [32, 64, 112, 112]        [32, 64, 112, 112]        True\n","│    └─ReLU: 2-3                         [32, 64, 112, 112]        [32, 64, 112, 112]        --\n","│    └─MaxPool2d: 2-4                    [32, 64, 112, 112]        [32, 64, 56, 56]          --\n","│    └─_DenseBlock: 2-5                  [32, 64, 56, 56]          [32, 256, 56, 56]         True\n","│    │    └─_DenseLayer: 3-1             [32, 64, 56, 56]          [32, 32, 56, 56]          True\n","│    │    └─_DenseLayer: 3-2             [32, 64, 56, 56]          [32, 32, 56, 56]          True\n","│    │    └─_DenseLayer: 3-3             [32, 64, 56, 56]          [32, 32, 56, 56]          True\n","│    │    └─_DenseLayer: 3-4             [32, 64, 56, 56]          [32, 32, 56, 56]          True\n","│    │    └─_DenseLayer: 3-5             [32, 64, 56, 56]          [32, 32, 56, 56]          True\n","│    │    └─_DenseLayer: 3-6             [32, 64, 56, 56]          [32, 32, 56, 56]          True\n","│    └─_Transition: 2-6                  [32, 256, 56, 56]         [32, 128, 28, 28]         True\n","│    │    └─BatchNorm2d: 3-7             [32, 256, 56, 56]         [32, 256, 56, 56]         True\n","│    │    └─ReLU: 3-8                    [32, 256, 56, 56]         [32, 256, 56, 56]         --\n","│    │    └─Conv2d: 3-9                  [32, 256, 56, 56]         [32, 128, 56, 56]         True\n","│    │    └─AvgPool2d: 3-10              [32, 128, 56, 56]         [32, 128, 28, 28]         --\n","│    └─_DenseBlock: 2-7                  [32, 128, 28, 28]         [32, 512, 28, 28]         True\n","│    │    └─_DenseLayer: 3-11            [32, 128, 28, 28]         [32, 32, 28, 28]          True\n","│    │    └─_DenseLayer: 3-12            [32, 128, 28, 28]         [32, 32, 28, 28]          True\n","│    │    └─_DenseLayer: 3-13            [32, 128, 28, 28]         [32, 32, 28, 28]          True\n","│    │    └─_DenseLayer: 3-14            [32, 128, 28, 28]         [32, 32, 28, 28]          True\n","│    │    └─_DenseLayer: 3-15            [32, 128, 28, 28]         [32, 32, 28, 28]          True\n","│    │    └─_DenseLayer: 3-16            [32, 128, 28, 28]         [32, 32, 28, 28]          True\n","│    │    └─_DenseLayer: 3-17            [32, 128, 28, 28]         [32, 32, 28, 28]          True\n","│    │    └─_DenseLayer: 3-18            [32, 128, 28, 28]         [32, 32, 28, 28]          True\n","│    │    └─_DenseLayer: 3-19            [32, 128, 28, 28]         [32, 32, 28, 28]          True\n","│    │    └─_DenseLayer: 3-20            [32, 128, 28, 28]         [32, 32, 28, 28]          True\n","│    │    └─_DenseLayer: 3-21            [32, 128, 28, 28]         [32, 32, 28, 28]          True\n","│    │    └─_DenseLayer: 3-22            [32, 128, 28, 28]         [32, 32, 28, 28]          True\n","│    └─_Transition: 2-8                  [32, 512, 28, 28]         [32, 256, 14, 14]         True\n","│    │    └─BatchNorm2d: 3-23            [32, 512, 28, 28]         [32, 512, 28, 28]         True\n","│    │    └─ReLU: 3-24                   [32, 512, 28, 28]         [32, 512, 28, 28]         --\n","│    │    └─Conv2d: 3-25                 [32, 512, 28, 28]         [32, 256, 28, 28]         True\n","│    │    └─AvgPool2d: 3-26              [32, 256, 28, 28]         [32, 256, 14, 14]         --\n","│    └─_DenseBlock: 2-9                  [32, 256, 14, 14]         [32, 1792, 14, 14]        True\n","│    │    └─_DenseLayer: 3-27            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-28            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-29            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-30            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-31            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-32            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-33            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-34            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-35            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-36            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-37            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-38            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-39            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-40            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-41            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-42            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-43            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-44            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-45            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-46            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-47            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-48            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-49            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-50            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-51            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-52            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-53            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-54            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-55            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-56            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-57            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-58            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-59            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-60            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-61            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-62            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-63            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-64            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-65            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-66            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-67            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-68            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-69            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-70            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-71            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-72            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-73            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    │    └─_DenseLayer: 3-74            [32, 256, 14, 14]         [32, 32, 14, 14]          True\n","│    └─_Transition: 2-10                 [32, 1792, 14, 14]        [32, 896, 7, 7]           True\n","│    │    └─BatchNorm2d: 3-75            [32, 1792, 14, 14]        [32, 1792, 14, 14]        True\n","│    │    └─ReLU: 3-76                   [32, 1792, 14, 14]        [32, 1792, 14, 14]        --\n","│    │    └─Conv2d: 3-77                 [32, 1792, 14, 14]        [32, 896, 14, 14]         True\n","│    │    └─AvgPool2d: 3-78              [32, 896, 14, 14]         [32, 896, 7, 7]           --\n","│    └─_DenseBlock: 2-11                 [32, 896, 7, 7]           [32, 1920, 7, 7]          True\n","│    │    └─_DenseLayer: 3-79            [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-80            [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-81            [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-82            [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-83            [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-84            [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-85            [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-86            [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-87            [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-88            [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-89            [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-90            [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-91            [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-92            [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-93            [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-94            [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-95            [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-96            [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-97            [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-98            [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-99            [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-100           [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-101           [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-102           [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-103           [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-104           [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-105           [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-106           [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-107           [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-108           [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-109           [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    │    └─_DenseLayer: 3-110           [32, 896, 7, 7]           [32, 32, 7, 7]            True\n","│    └─BatchNorm2d: 2-12                 [32, 1920, 7, 7]          [32, 1920, 7, 7]          True\n","├─Linear: 1-2                            [32, 1920]                [32, 1000]                True\n","===================================================================================================================\n","Total params: 20,013,928\n","Trainable params: 20,013,928\n","Non-trainable params: 0\n","Total mult-adds (G): 137.33\n","===================================================================================================================\n","Input size (MB): 19.27\n","Forward/backward pass size (MB): 8335.09\n","Params size (MB): 80.06\n","Estimated Total Size (MB): 8434.42\n","==================================================================================================================="]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["summary(densenet,input_size = [32,3,224,224],col_names=[\"input_size\",\"output_size\",\"trainable\"])"]},{"cell_type":"markdown","metadata":{},"source":["Since we are not going to retrain the entire model, we freeze the feature layers in the pretrained model so that we only focus on training the classifier in the final layers of the model. The feature layers will be useful in identifying general image characteristics. "]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T12:38:54.000062Z","iopub.status.busy":"2024-02-17T12:38:53.999690Z","iopub.status.idle":"2024-02-17T12:38:54.475298Z","shell.execute_reply":"2024-02-17T12:38:54.474116Z","shell.execute_reply.started":"2024-02-17T12:38:54.000033Z"},"trusted":true},"outputs":[{"data":{"text/plain":["===================================================================================================================\n","Layer (type:depth-idx)                   Input Shape               Output Shape              Trainable\n","===================================================================================================================\n","DenseNet                                 [32, 3, 224, 224]         [32, 1000]                Partial\n","├─Sequential: 1-1                        [32, 3, 224, 224]         [32, 1920, 7, 7]          False\n","│    └─Conv2d: 2-1                       [32, 3, 224, 224]         [32, 64, 112, 112]        False\n","│    └─BatchNorm2d: 2-2                  [32, 64, 112, 112]        [32, 64, 112, 112]        False\n","│    └─ReLU: 2-3                         [32, 64, 112, 112]        [32, 64, 112, 112]        --\n","│    └─MaxPool2d: 2-4                    [32, 64, 112, 112]        [32, 64, 56, 56]          --\n","│    └─_DenseBlock: 2-5                  [32, 64, 56, 56]          [32, 256, 56, 56]         False\n","│    │    └─_DenseLayer: 3-1             [32, 64, 56, 56]          [32, 32, 56, 56]          False\n","│    │    └─_DenseLayer: 3-2             [32, 64, 56, 56]          [32, 32, 56, 56]          False\n","│    │    └─_DenseLayer: 3-3             [32, 64, 56, 56]          [32, 32, 56, 56]          False\n","│    │    └─_DenseLayer: 3-4             [32, 64, 56, 56]          [32, 32, 56, 56]          False\n","│    │    └─_DenseLayer: 3-5             [32, 64, 56, 56]          [32, 32, 56, 56]          False\n","│    │    └─_DenseLayer: 3-6             [32, 64, 56, 56]          [32, 32, 56, 56]          False\n","│    └─_Transition: 2-6                  [32, 256, 56, 56]         [32, 128, 28, 28]         False\n","│    │    └─BatchNorm2d: 3-7             [32, 256, 56, 56]         [32, 256, 56, 56]         False\n","│    │    └─ReLU: 3-8                    [32, 256, 56, 56]         [32, 256, 56, 56]         --\n","│    │    └─Conv2d: 3-9                  [32, 256, 56, 56]         [32, 128, 56, 56]         False\n","│    │    └─AvgPool2d: 3-10              [32, 128, 56, 56]         [32, 128, 28, 28]         --\n","│    └─_DenseBlock: 2-7                  [32, 128, 28, 28]         [32, 512, 28, 28]         False\n","│    │    └─_DenseLayer: 3-11            [32, 128, 28, 28]         [32, 32, 28, 28]          False\n","│    │    └─_DenseLayer: 3-12            [32, 128, 28, 28]         [32, 32, 28, 28]          False\n","│    │    └─_DenseLayer: 3-13            [32, 128, 28, 28]         [32, 32, 28, 28]          False\n","│    │    └─_DenseLayer: 3-14            [32, 128, 28, 28]         [32, 32, 28, 28]          False\n","│    │    └─_DenseLayer: 3-15            [32, 128, 28, 28]         [32, 32, 28, 28]          False\n","│    │    └─_DenseLayer: 3-16            [32, 128, 28, 28]         [32, 32, 28, 28]          False\n","│    │    └─_DenseLayer: 3-17            [32, 128, 28, 28]         [32, 32, 28, 28]          False\n","│    │    └─_DenseLayer: 3-18            [32, 128, 28, 28]         [32, 32, 28, 28]          False\n","│    │    └─_DenseLayer: 3-19            [32, 128, 28, 28]         [32, 32, 28, 28]          False\n","│    │    └─_DenseLayer: 3-20            [32, 128, 28, 28]         [32, 32, 28, 28]          False\n","│    │    └─_DenseLayer: 3-21            [32, 128, 28, 28]         [32, 32, 28, 28]          False\n","│    │    └─_DenseLayer: 3-22            [32, 128, 28, 28]         [32, 32, 28, 28]          False\n","│    └─_Transition: 2-8                  [32, 512, 28, 28]         [32, 256, 14, 14]         False\n","│    │    └─BatchNorm2d: 3-23            [32, 512, 28, 28]         [32, 512, 28, 28]         False\n","│    │    └─ReLU: 3-24                   [32, 512, 28, 28]         [32, 512, 28, 28]         --\n","│    │    └─Conv2d: 3-25                 [32, 512, 28, 28]         [32, 256, 28, 28]         False\n","│    │    └─AvgPool2d: 3-26              [32, 256, 28, 28]         [32, 256, 14, 14]         --\n","│    └─_DenseBlock: 2-9                  [32, 256, 14, 14]         [32, 1792, 14, 14]        False\n","│    │    └─_DenseLayer: 3-27            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-28            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-29            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-30            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-31            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-32            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-33            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-34            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-35            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-36            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-37            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-38            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-39            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-40            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-41            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-42            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-43            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-44            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-45            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-46            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-47            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-48            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-49            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-50            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-51            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-52            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-53            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-54            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-55            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-56            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-57            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-58            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-59            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-60            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-61            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-62            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-63            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-64            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-65            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-66            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-67            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-68            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-69            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-70            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-71            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-72            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-73            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    │    └─_DenseLayer: 3-74            [32, 256, 14, 14]         [32, 32, 14, 14]          False\n","│    └─_Transition: 2-10                 [32, 1792, 14, 14]        [32, 896, 7, 7]           False\n","│    │    └─BatchNorm2d: 3-75            [32, 1792, 14, 14]        [32, 1792, 14, 14]        False\n","│    │    └─ReLU: 3-76                   [32, 1792, 14, 14]        [32, 1792, 14, 14]        --\n","│    │    └─Conv2d: 3-77                 [32, 1792, 14, 14]        [32, 896, 14, 14]         False\n","│    │    └─AvgPool2d: 3-78              [32, 896, 14, 14]         [32, 896, 7, 7]           --\n","│    └─_DenseBlock: 2-11                 [32, 896, 7, 7]           [32, 1920, 7, 7]          False\n","│    │    └─_DenseLayer: 3-79            [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-80            [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-81            [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-82            [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-83            [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-84            [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-85            [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-86            [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-87            [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-88            [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-89            [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-90            [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-91            [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-92            [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-93            [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-94            [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-95            [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-96            [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-97            [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-98            [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-99            [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-100           [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-101           [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-102           [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-103           [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-104           [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-105           [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-106           [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-107           [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-108           [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-109           [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    │    └─_DenseLayer: 3-110           [32, 896, 7, 7]           [32, 32, 7, 7]            False\n","│    └─BatchNorm2d: 2-12                 [32, 1920, 7, 7]          [32, 1920, 7, 7]          False\n","├─Linear: 1-2                            [32, 1920]                [32, 1000]                True\n","===================================================================================================================\n","Total params: 20,013,928\n","Trainable params: 1,921,000\n","Non-trainable params: 18,092,928\n","Total mult-adds (G): 137.33\n","===================================================================================================================\n","Input size (MB): 19.27\n","Forward/backward pass size (MB): 8335.09\n","Params size (MB): 80.06\n","Estimated Total Size (MB): 8434.42\n","==================================================================================================================="]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\n","for param in densenet.features.parameters():\n","    param.requires_grad = False\n","# Check summary again after\n","summary(densenet, input_size= [32,3,224,224],col_names= [\"input_size\",\"output_size\",\"trainable\"])"]},{"cell_type":"markdown","metadata":{},"source":["# Setting up the classifier for training"]},{"cell_type":"markdown","metadata":{},"source":["Finally we define the random seed and the classifier of our new model."]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T12:39:20.552016Z","iopub.status.busy":"2024-02-17T12:39:20.551636Z","iopub.status.idle":"2024-02-17T12:39:20.559585Z","shell.execute_reply":"2024-02-17T12:39:20.558506Z","shell.execute_reply.started":"2024-02-17T12:39:20.551990Z"},"trusted":true},"outputs":[],"source":["# Set the manual seeds\n","torch.manual_seed(42)\n","torch.cuda.manual_seed(42)\n","\n","# Get the length of class_names (one output unit for each class)\n","output_shape = len(class_names)\n","\n","# Recreate the classifier layer and seed it to the target device\n","densenet.classifier = torch.nn.Sequential(\n","    torch.nn.Dropout(p=0.2, inplace=True),\n","    torch.nn.Linear(in_features=1920,\n","                    out_features=output_shape, # same number of output units as our number of classes\n","                    bias=True)).to(device)"]},{"cell_type":"markdown","metadata":{},"source":["# Loss Function and Optimizer"]},{"cell_type":"markdown","metadata":{},"source":["We also define the loss function also known as criterion and the optimizer. Adam is a popular optimizer and the go to loss function for classification (multiclass) is categorical cross entropy loss, hence the choices we  made. "]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T12:39:26.350519Z","iopub.status.busy":"2024-02-17T12:39:26.349933Z","iopub.status.idle":"2024-02-17T12:39:26.360146Z","shell.execute_reply":"2024-02-17T12:39:26.359163Z","shell.execute_reply.started":"2024-02-17T12:39:26.350468Z"},"trusted":true},"outputs":[],"source":["# Define loss and optimizer\n","loss_fn = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(densenet.parameters(), lr=0.001)"]},{"cell_type":"markdown","metadata":{},"source":["We also define the number of epochs. This is an important hyperparameter that can be tweaked depending on the model performance."]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T12:55:15.831712Z","iopub.status.busy":"2024-02-17T12:55:15.831107Z","iopub.status.idle":"2024-02-17T12:55:15.835837Z","shell.execute_reply":"2024-02-17T12:55:15.834755Z","shell.execute_reply.started":"2024-02-17T12:55:15.831681Z"},"trusted":true},"outputs":[],"source":["epochs = 10"]},{"cell_type":"markdown","metadata":{},"source":["# Initiate wandb"]},{"cell_type":"markdown","metadata":{},"source":["We initiate wandb so that we can properly track the model performance. "]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T12:55:22.869053Z","iopub.status.busy":"2024-02-17T12:55:22.868177Z","iopub.status.idle":"2024-02-17T12:55:53.700136Z","shell.execute_reply":"2024-02-17T12:55:53.699071Z","shell.execute_reply.started":"2024-02-17T12:55:22.869018Z"},"trusted":true},"outputs":[{"data":{"text/html":["wandb version 0.16.3 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.2"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240217_125522-bwgfk4yt</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/lukmanaj/arewads_capstone/runs/bwgfk4yt' target=\"_blank\">legendary-cake-3</a></strong> to <a href='https://wandb.ai/lukmanaj/arewads_capstone' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/lukmanaj/arewads_capstone' target=\"_blank\">https://wandb.ai/lukmanaj/arewads_capstone</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/lukmanaj/arewads_capstone/runs/bwgfk4yt' target=\"_blank\">https://wandb.ai/lukmanaj/arewads_capstone/runs/bwgfk4yt</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/lukmanaj/arewads_capstone/runs/bwgfk4yt?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7cf8477400a0>"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["\n","\n","# Initialize wandb\n","wandb.init(project='arewads_capstone', config={\n","  \"learning_rate\": optimizer.param_groups[0]['lr'],\n","  \"epochs\": epochs,\n","  \"batch_size\": train_dataloader.batch_size\n","})\n"]},{"cell_type":"markdown","metadata":{},"source":["# Actual Training"]},{"cell_type":"markdown","metadata":{},"source":["The training is done in this few lines of code thanks to the earlier leg work that was done. "]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T12:56:34.686866Z","iopub.status.busy":"2024-02-17T12:56:34.686179Z","iopub.status.idle":"2024-02-17T12:58:10.264660Z","shell.execute_reply":"2024-02-17T12:58:10.263871Z","shell.execute_reply.started":"2024-02-17T12:56:34.686832Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5d98024527894ced8f76278938d19225","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 1 | train_loss: 0.4878 | train_acc: 0.8125 | test_loss: 0.8406 | test_acc: 0.6794\n","Epoch: 2 | train_loss: 0.4994 | train_acc: 0.8077 | test_loss: 0.8984 | test_acc: 0.6771\n","Epoch: 3 | train_loss: 0.4735 | train_acc: 0.8285 | test_loss: 0.8422 | test_acc: 0.6725\n","Epoch: 4 | train_loss: 0.4763 | train_acc: 0.8277 | test_loss: 0.8219 | test_acc: 0.6782\n","Epoch: 5 | train_loss: 0.5007 | train_acc: 0.8189 | test_loss: 0.8737 | test_acc: 0.6887\n","Epoch: 6 | train_loss: 0.5281 | train_acc: 0.7933 | test_loss: 0.8761 | test_acc: 0.6655\n","Epoch: 7 | train_loss: 0.4820 | train_acc: 0.8253 | test_loss: 0.9172 | test_acc: 0.6586\n","Epoch: 8 | train_loss: 0.5257 | train_acc: 0.8005 | test_loss: 0.8662 | test_acc: 0.6678\n","Epoch: 9 | train_loss: 0.4777 | train_acc: 0.8189 | test_loss: 0.9217 | test_acc: 0.6655\n","Epoch: 10 | train_loss: 0.4832 | train_acc: 0.8053 | test_loss: 0.9086 | test_acc: 0.6377\n","[INFO] Total training time: 90.516 seconds\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_acc</td><td>▇▆▆▇█▅▄▅▅▁</td></tr><tr><td>test_loss</td><td>▂▆▂▁▅▅█▄█▇</td></tr><tr><td>train_acc</td><td>▅▄██▆▁▇▂▆▃</td></tr><tr><td>train_loss</td><td>▃▄▁▁▄█▂█▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_acc</td><td>0.63773</td></tr><tr><td>test_loss</td><td>0.90856</td></tr><tr><td>train_acc</td><td>0.80529</td></tr><tr><td>train_loss</td><td>0.48315</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">legendary-cake-3</strong> at: <a href='https://wandb.ai/lukmanaj/arewads_capstone/runs/bwgfk4yt' target=\"_blank\">https://wandb.ai/lukmanaj/arewads_capstone/runs/bwgfk4yt</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240217_125522-bwgfk4yt/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Set the random seeds\n","torch.manual_seed(42)\n","torch.cuda.manual_seed(42)\n","\n","# Start the timer\n","from timeit import default_timer as timer\n","start_time = timer()\n","\n","# Setup training and save the results\n","densenet_results = train(model=densenet,\n","                       train_dataloader=train_dataloader,\n","                       test_dataloader=test_dataloader,\n","                       optimizer=optimizer,\n","                       loss_fn=loss_fn,\n","                       epochs=epochs,\n","                       device=device)\n","\n","# End the timer and print out how long it took\n","end_time = timer()\n","print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")\n","# Properly close the wandb session\n","wandb.finish()"]},{"cell_type":"markdown","metadata":{},"source":["# Saving the model\n"]},{"cell_type":"markdown","metadata":{},"source":["The next thing is to save the model since we intend to use it for the app. After all, the accuracy is fairly okay. The `save_model` function is defined below in the modular code spirit. "]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T12:58:24.287907Z","iopub.status.busy":"2024-02-17T12:58:24.287125Z","iopub.status.idle":"2024-02-17T12:58:24.294961Z","shell.execute_reply":"2024-02-17T12:58:24.293914Z","shell.execute_reply.started":"2024-02-17T12:58:24.287871Z"},"trusted":true},"outputs":[],"source":["# Saving the model\n","\n","\n","def save_model(model: torch.nn.Module,\n","               target_dir: str,\n","               model_name: str):\n","    \"\"\"Saves a PyTorch model to a target directory.\n","\n","    Args:\n","    model: A target PyTorch model to save.\n","    target_dir: A directory for saving the model to.\n","    model_name: A filename for the saved model. Should include\n","      either \".pth\" or \".pt\" as the file extension.\n","\n","    Example usage:\n","    save_model(model=model_0,\n","               target_dir=\"models\",\n","               model_name=\"05_going_modular_tingvgg_model.pth\")\n","    \"\"\"\n","    # Create target directory\n","    target_dir_path = Path(target_dir)\n","    target_dir_path.mkdir(parents=True,\n","                        exist_ok=True)\n","\n","    # Create model save path\n","    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n","    model_save_path = target_dir_path / model_name\n","\n","    # Save the model state_dict()\n","    print(f\"[INFO] Saving model to: {model_save_path}\")\n","    torch.save(obj=model.state_dict(),\n","             f=model_save_path)"]},{"cell_type":"markdown","metadata":{},"source":["Model is saved and we give it a befitting name. "]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T12:58:33.864213Z","iopub.status.busy":"2024-02-17T12:58:33.863731Z","iopub.status.idle":"2024-02-17T12:58:34.098401Z","shell.execute_reply":"2024-02-17T12:58:34.097400Z","shell.execute_reply.started":"2024-02-17T12:58:33.864176Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[INFO] Saving model to: models/densenetafri2.pth\n"]}],"source":["save_model(model=densenet,target_dir='models',model_name='densenetafri2.pth')"]},{"cell_type":"markdown","metadata":{},"source":["# Moment of truth"]},{"cell_type":"markdown","metadata":{},"source":["And the moment of truth, seeing if the weights can be used on a fresh downloaded version of densenet201"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T12:58:41.288720Z","iopub.status.busy":"2024-02-17T12:58:41.288330Z","iopub.status.idle":"2024-02-17T12:58:42.138502Z","shell.execute_reply":"2024-02-17T12:58:42.137452Z","shell.execute_reply.started":"2024-02-17T12:58:41.288692Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["# Setup the model with pretrained weights and send it to the target device\n","weights = torchvision.models.DenseNet201_Weights.DEFAULT # best available weight\n","#transform = torchvision.models.AlexNet_Weights.IMAGENET1K_V1.transforms \n","densenet2 = torchvision.models.densenet201(weights=weights).to(device)\n","\n","# Get the length of class_names (one output unit for each class)\n","output_shape = len(class_names)\n","\n","# Recreate the classifier layer and seed it to the target device\n","densenet2.classifier = torch.nn.Sequential(\n","    torch.nn.Dropout(p=0.2, inplace=True),\n","    torch.nn.Linear(in_features=1920,\n","                    out_features=output_shape, # same number of output units as our number of classes\n","                    bias=True)).to(device)\n","\n","model_path = '/kaggle/working/models/densenetafri2.pth'\n","\n","# Load the saved model state dictionary into the model\n","densenet2.load_state_dict(torch.load(model_path))\n"]},{"cell_type":"markdown","metadata":{},"source":["And, it's a success. The hardwork paid off. "]},{"cell_type":"markdown","metadata":{},"source":["# Finally Prediction on New Data\n","\n","In the final section of the notebook, we predict on a newly downloaded image from the internet."]},{"cell_type":"markdown","metadata":{},"source":["First the paths to the model and the picture are defined."]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T12:58:46.556859Z","iopub.status.busy":"2024-02-17T12:58:46.556427Z","iopub.status.idle":"2024-02-17T12:58:46.561580Z","shell.execute_reply":"2024-02-17T12:58:46.560536Z","shell.execute_reply.started":"2024-02-17T12:58:46.556829Z"},"trusted":true},"outputs":[],"source":["image_path = '/kaggle/input/random-picture/pic.jpg'\n","model_path = '/kaggle/working/models/densenetafri.pth'"]},{"cell_type":"markdown","metadata":{},"source":["Then we define functions that allow us to load the model and predict on a new image."]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T12:58:49.881487Z","iopub.status.busy":"2024-02-17T12:58:49.881112Z","iopub.status.idle":"2024-02-17T12:58:49.894165Z","shell.execute_reply":"2024-02-17T12:58:49.893147Z","shell.execute_reply.started":"2024-02-17T12:58:49.881447Z"},"trusted":true},"outputs":[],"source":["def load_model(model_path, device):\n","    \"\"\"\n","    Loads a model from the specified file path.\n","\n","    Args:\n","    model_path (str): Path to the model file.\n","    device (str): The device to load the model on ('cuda' or 'cpu').\n","\n","    Returns:\n","    torch.nn.Module: Loaded PyTorch model.\n","    \"\"\"\n","    weights = torchvision.models.DenseNet201_Weights.DEFAULT # best available weight\n","    # Recreate the classifier layer and seed it to the target device\n","    model = torchvision.models.densenet201(weights=weights).to(device)\n","    model.classifier = torch.nn.Sequential(\n","    torch.nn.Dropout(p=0.2, inplace=True),\n","    torch.nn.Linear(in_features=1920,\n","                    out_features=4, # same number of output units as our number of classes\n","                    bias=True)).to(device)\n","    model.load_state_dict(torch.load(model_path, map_location=device))\n","    model.to(device)\n","    model.eval()\n","    return model\n","\n","def preprocess_image(image_path):\n","    \"\"\"\n","    Preprocesses the image for model prediction.\n","\n","    Args:\n","    image_path (str): Path to the image file.\n","\n","    Returns:\n","    torch.Tensor: Preprocessed image tensor.\n","    \"\"\"\n","    image = torchvision.io.read_image(image_path).type(torch.float32)\n","    image = image / 255.0  # Normalize to [0, 1]\n","    transform = transforms.Resize(size=(64, 64))\n","    return transform(image)\n","\n","def get_prediction(model, image, device):\n","    \"\"\"\n","    Predicts the class for the given image using the specified model.\n","\n","    Args:\n","    model (torch.nn.Module): The trained model for prediction.\n","    image (torch.Tensor): The preprocessed image tensor.\n","    device (str): The device to perform prediction on ('cuda' or 'cpu').\n","\n","    Returns:\n","    str, float: Predicted class name and the probability.\n","    \"\"\"\n","    class_names = ['buffalo', 'elephant', 'rhino', 'zebra']\n","    image = image.unsqueeze(0).to(device)  # Add batch dimension and move to device\n","    with torch.inference_mode():\n","        pred_logits = model(image)\n","        pred_prob = torch.softmax(pred_logits, dim=1)\n","        pred_label = torch.argmax(pred_prob, dim=1)\n","    return class_names[pred_label], pred_prob.max().item()\n","\n","def predict_image(image_path, model_path):\n","    \"\"\"\n","    Main function to handle model prediction on the given image.\n","\n","    Args:\n","    image_path (str): Path to the target image.\n","    model_path (str): Path to the trained model file.\n","    \"\"\"\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    print(f\"[INFO] Predicting on image: {image_path}\")\n","\n","    # Load the model and preprocess the image\n","    model = load_model(model_path, device)\n","    image = preprocess_image(image_path)\n","\n","    # Predict and print the result\n","    pred_class, pred_prob = get_prediction(model, image, device)\n","    print(f\"[INFO] Predicted class: {pred_class}, Probability: {pred_prob:.3f}\")"]},{"cell_type":"markdown","metadata":{},"source":["And lastly we predict on the image"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T12:58:55.707452Z","iopub.status.busy":"2024-02-17T12:58:55.707079Z","iopub.status.idle":"2024-02-17T12:58:56.587587Z","shell.execute_reply":"2024-02-17T12:58:56.586678Z","shell.execute_reply.started":"2024-02-17T12:58:55.707423Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[INFO] Predicting on image: /kaggle/input/random-picture/pic.jpg\n","[INFO] Predicted class: buffalo, Probability: 0.634\n"]}],"source":["predict_image(image_path,'/kaggle/working/models/densenetafri.pth')"]},{"cell_type":"markdown","metadata":{},"source":["And it predicted the right class of the animal. Yay. Our small model works. "]},{"cell_type":"markdown","metadata":{},"source":["# Deployed on Streamlit : Check it out"]},{"cell_type":"markdown","metadata":{},"source":["Seeing our small model works, we deployed it on streamlit. \n","Kindly find it [here](https://wildlifeclassify.streamlit.app/) and give feedback on the performance. "]},{"cell_type":"markdown","metadata":{},"source":["If you found this notebook helpful, kindly give it a thumbs up. "]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":674157,"sourceId":1185810,"sourceType":"datasetVersion"},{"datasetId":4445056,"sourceId":7629314,"sourceType":"datasetVersion"}],"dockerImageVersionId":30648,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
